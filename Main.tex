\documentclass[a4paper,12pt]{scrartcl}


\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{hyperref}

\title{Homework 01}
\author{Yesid Cano Castro, Moritz LÃ¶nker and Tim Niklas Witte}
\date{}

\begin{document}

{
\let\clearpage\relax
\maketitle
}

\section*{Task 01}


You are tasked with creating an AI for the game of chess. To solve the problem
using Reinforcement Learning, you have to frame the game of chess as a Markov
Decision Process (MDP). Describe both the game of chess formally as a MDP,
also formalize the respective policy.

\subsection*{Solution}

\begin{itemize}
 \item Set of states $S^{ \textrm{Num rows} \times \textrm{Num columns} \times \textrm{Chess pieces} }$.
 \item Set of actions $A$: Let be $d = \{\textrm{up}, \textrm{down}, \textrm{right}, \textrm{left}\dots\}$.
  $A = \bigcup\limits_{i=1}^{\textrm{Chess pieces}} \hat{A_i}$ with $\hat{A_i} = \{x : x\in d \land \textrm{isAvaibleAction(x, i)} \}$. 
  \item State dynamics/state transition function $p(s'|s,a) = \textrm{makeMove(s,a)}$. $\textrm{makeMove(s,a)}$ returns a next state given action $a$ and current state $s$.
  \item Reward dynamics $p(R_{t+1}|s,a) = \textrm{killEnemyPiece(s,a)}$.
  
  $
  \textrm{killEnemyPiece(s,a)} =
  \left\{
	\begin{array}{ll}
		1  & \mbox{if } \textrm{action $a$ does capture a enemy piece in current state $s$} \\
		0 & \mbox{otherwise }
	\end{array}
\right.
$
  \item Initial state $\mu = \textrm{start state} \in S$.
  
  \item Policy: $\pi(s) = \underset{a \in A}{\arg\max} \ V_\pi(s)$ with $V_\pi(s) = \textrm{Number of captured chess pieces from the enemy}$.   
\end{itemize}

\section*{Task 02}
Check out the LunarLander environment on OpenAI Gym: Check out this Link!.
Describe the environment as a MDP, include a description how the policy is
formalized.

\begin{itemize}

\item Sate space: set of 8-dimensional sate vectors where each dimension corresponds to:
\begin{itemize}
    \item s[0] is the horizontal coordinate
    \item s[1] is the vertical coordinate
    \item s[2] is the horizontal speed
    \item s[3] is the vertical speed
    \item s[4] is the angle
    \item s[5] is the angular speed
    \item s[6] 1 if first leg has contact, else 0
    \item s[7] 1 if second leg has contact, else 0
\end{itemize}
\item Action space:
\begin{itemize}
    \item Do nothing
    \item Fire left orientation engine
    \item Fire main engine
    \item Fire right orientation engine

\end{itemize}
\item Rewards
\begin{itemize}
    \item Moving from the top of the screen to the landing pad and coming.
    to rest is about 100-140 points.
    \item If the lander moves away from the landing pad, it loses reward.
    \item If the lander crashes, it receives an additional -100 points.
    \item If the lander comes to rest, it receives an additional +100 points.
    \item Each lander's leg with ground contact is +10 points.
    \item Firing the main engine is -0.3 points each frame.
    \item Firing the side engine is -0.03 points each frame.
    \item Solved is 200 points.
\end{itemize}
\item{Formalize a policy:}
Since there is a total of four actions, one needs to assign a probability to each of them, for example, 0.25 to each of them. In this case, the system is initialized with a random policy.
\item{Transition:}
$p(s'|s,a) = 1$; the probability that action $a$ in state $s$ will lead to state $s'$ is one.

\end{itemize}
Answers were taken from this \href{https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py} {repository}

\section*{Task 03}
Discuss the Policy Evaluation and Policy Iteration algorithms from the lecture.
They explicitly make use of the environment dynamics ($p(s', r|s, a)$).
\begin{itemize}
\item Explain what the environment dynamics (i.e. reward function and state
transition function) are and give at least two examples.
\item Discuss: Are the environment dynamics generally known and can practi-
cally be used to solve a problem with RL?
\end{itemize}

\subsection*{Solution}

The reward function is defined as the expected value of the reward which is gained given a state $s$, in which the agent performs action $a$.\\
\indent Example 1: $r(s,a) = killEnemyPiece(s,a)$ for chess\\
\indent Example 2: \\\\
The state transition function is defined as the probability of reaching state $s'$ given a state $s$, in which the agent performs action $a$. \\
\indent Example 1: $p(s'|s,a) = makeMove(s,a)$ for chess\\
\indent Example 2:\\\\
Generally the environment dynamics are not completely known, if they are completely known this is certainly beneficial, as then it would be possible to calculate an optimal policy given enough computing power. In practise this is of course often not applicable, because of the exponential rise in necessary computing power that comes with many applications (e.g. chess).

\end{document}